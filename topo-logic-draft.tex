\documentclass[hoptionsi,review,format=acmsmall]{acmart}
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex
\usepackage{amssymb}
\usepackage{xypic}
\usepackage{cancel}
\usepackage{mathtools}
\newtheorem*{remark}{Remark}


% \newtheorem*{corollary}{Corollary}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

%SetFonts

%SetFonts

%------------------------------------------------
%            Symbols in "mathcal"
%------------------------------------------------

\newcommand{\Mcc}{\mathcal{M}}
\newcommand{\UBLc}{\mathcal{UBL}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Ucc}{\mathcal{U}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Lcc}{\mathcal{L}}
\newcommand{\Dcc}{\mathcal{D}}
\newcommand{\BLc}{\mathcal{BL}}

\newcommand{\band}{\mathbin{\&}}
\newcommand{\bor}{\mathbin{|}}
\newcommand{\Dia}{\diamondsuit}

% TODO pick font better than mathfrak for combinatorial operators

\newcommand{\Mf}{\mathfrak{m}} 
\newcommand{\Sf}{\mathfrak{s}}
\newcommand{\Wf}{\mathfrak{w}}
\newcommand{\Hf}{\mathfrak{h}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title[Concurrency and Dependency via Distributive Lattices]{The Topological and Logical Structure of Concurrency and Dependency via Distributive Lattices}
\author{Gershom Bazerman}
 \affiliation{%
   \institution{Awake Security}}
 \email{gershomb@gmail.com}
\author{Raymond Puzio}
\affiliation{%
   \institution{Albert Einstein Institute}}
\email{rspuzio@gmail.com}

% TODO TODO FIX QUOTES

\begin{abstract}
It is widely recognized as desirable to give a mathematical semantics to specifications of concurrent computations. But the purpose of a semantics is not just to exist, but to be useful in further analyzing the properties of the programs they provide models of. So it is desirable in particular to give a semantics that has familiar logical properties to facilitate reasoning, but also tractable topological properties, so that the weight of modern mathematical analysis may be brought to bear on teasing out important structural aspects. This paper makes a number of related contributions in this regard. First, it relates the specification of branching dependency structures, which exist in fields from knowledge-representation to package management, to the specification of semantics of concurrent computation. Second, it relates dependency structures to lattices in a precise way. It then makes use of this as a key ingredient, coupled with the underappreciated Bruns-Lakser completion, in relating dependency structures to locales -- objects equipped with both topological and logical properties. It then provides an example of how this interplay of properties can be of use -- using topological properties of the dependency structure to equip internal logics of associated locales with a modality representing contraction relations (i.e. "versioning"). Finally, it discusses how such constructions may relate to important questions in complexity theory regarding SAT-solvers. Along the way, we will see how this approach relates to familiar objects such as package version policies, Merkle-trees, the nix operating system, and distributed version control tooling like git.
\end{abstract}

\begin{document}
\maketitle

\section{Introduction}
This project began with seeking to understand the mathematical structure and logic of software package repositories. Such repositories contain tens of thousands of packages, with complex webs of interlinking dependencies, represented as expressions in propositional logic, containing not only branching choices, but also a notion of "compatibility ranges" and a notion of conflict. The structures we developed for modeling this turned out to be extremely similar to work on the semantics of concurrent computation -- and for good reason! The problem of branching dependency specification is exactly the problem of concurrent computation, just "turned on its head." Semantics of concurrent computation, at base, consist of a collection of states, and certain allowable transitions between them, which may be simultaneous, and which may be nondeterministic. A dependency specification, such as given by a package repository, also has a collection of states (the collection of installed packages), and also has a collection of allowable transitions (one may only install a new package when all dependencies are satisfied). Furthermore, concurrency takes the form of distinguishing when two independent packages may be built simultaneously. And finally, in both cases, we have a notion of "incompatibility" -- the former, in terms of contention for a shared resource, and the latter, in terms of e.g. disallowing that two linked packages expose the same required symbol with different definitions. The difference is then largely in the questions asked about such structures. With concurrent semantics, the whole structure is the "program" and the typical questions asked are how such things compose. With dependency structures, a "program" is what we think of as a "build plan" -- a single trace through the structure to a particular end state, and the questions asked are about optimality, reachability, etc. Our approach is inspired by the latter way of thinking, but we think it sheds light on many related issues as well. In particular, we see that "inside" any single concurrent program, we can examine not only its state space, but also a related internal logic -- the logic of dependency specifications. This lends itself to fine-grained intensional analysis, exploiting the interplay.

In section 2, we introduce the basic elements and tools of our analysis -- dependency structures with choice, and their related "trace" structures, reachable dependency posets. Section 3 reviews the Bruns-Lakser completion, which is an independent order-theoretical result of general mathematical interest. We then discuss how reachable dependency posets, through the lens of Bruns-Lakser, produce a very general formal mathematical notion of what a "Merkle" structure is. Section 4 discusses two intuitionistic logics that can be built over this completion -- one an immediate "logic of paths", and one passing through the free distributive lattice over a poset to yield a "logic of requirements." Section 5 relates the topological and logical aspects of this investigation by examining how certain "covering" relations in dependency structures give rise to modalities in the associated logics discussed. Section 6 presents some preliminary investigations on how this approach may shed light on the innate topological structure of SAT problems, with regards to the difficulty they present to SAT solvers. We then conclude with a discussion on related and future work.

\section{Dependency Structures with Choice}
We begin our analysis with a definition of dependency structures. These are intended to correspond almost immediately to the data provided by package repositories -- events (packages) which may depend on a choice of other events. These structures do not (yet) have any notion of a choice of versions -- so version 1.0 and 1.1 of the same package are logically two entirely different events. Later we will see how to recover this data.

\begin{definition}
A \textbf{Pre-Dependency Structure with Choice} is a pair \((E, D : E \rightarrow \Pc(\Pc(E)))\) where \(E\) is a finite set of events, and \(D\) is a non-nullary mapping from \(E\) to its double powerset, to be interpreted as mapping each event to a set of alternative dependency requirements -- i.e. to a predicate in disjunctive normal form ranging over variables drawn from \(E\).
\end{definition}

\begin{definition}
A \textbf{Dependency Structure with Choice} (DSC) is a pre-DSC with \(D\) satisfying  appropriate conditions of transitive closure and cycle-freeness. We define \(X\) as a \textbf{possible dependency set} of \(e\) if \(X \in D(e)\). We call an event set \(X\) a \textbf{complete event set} if for every element \(e\) there is a possible dependency set \(Y\) of \(e\) such that \(Y \subseteq X\). A pre-DSC is a DSC if every possible dependency set of every element is complete, and no possible dependency set of any element contains the element itself. Pre-DSCs may be completed into DSCs by repeatedly taking transitive completion of possible dependency sets (with regards to each transitive possible dependency set) and then deleting cyclic sets (and elements whose possible dependency set becomes empty under this process) until a fixpoint is reached.
\end{definition}

DSCs are richer than the standard notion of a dependency tree or dependency graph. In such structures, a node \(a\) with edges to \(b\) and \(c\) exhibits a dependency on both \(b\) and \(c\). There is no way, however, to express a dependency on either \(b\) or \(c\). A domain-theoretic account of such structures is given by \textit{pomsets}, introduced by Vaughn Pratt \cite{pratt1986modeling}. Pomsets are a special instance of a broader class of structures, known as \textit{event structures}, introduced by Nielson, Plotkin and Winskel \cite{nielsen1981petri}, and used in the domain-theoretic semantics of concurrent computation and concurrent games. Such structures have not only a (choice-free, transitively normalized) dependency relation, but in addition a conflict structure which indicates incompatible collections of events (typically presented as a collection of consistent sets which carves out only compatible collections of events). Finally, there are so-called \textit{general event structures}, which extend event structures with a notion of choice in roughly the same fashion as DSCs (i.e. by moving from a partial ordering relation into a relation between elements and powersets). Their theory is less well behaved and understood, and its study is an area of ongoing work. DSCs may be characterized as general event structures which are conflict-free. Thus intuitively (i.e. not necessarily formally), in a lattice of expressive power, DSCs sit above pomsets, "side by side" with event structures, and below general event structures. One hope of the present is work is that it might be usefully extended in some fashion with conflicts, leading among other things to a further understanding of general event structures.\footnote{When first introduced, what we now know as "general event structures" were simply named "event structures", and what we now know as "event structures" were named "stable event structures." We follow the modern convention in this paper.}

\subsection{Reachable Dependencies Posets of a DSC}

Data as given in a DSC is purely declarative. To introduce an analysis of dynamics, we need a structure which we can trace through time. From DSCs we derive a partially ordered set of execution traces, analogous to the family of configurations of an event structure.

\begin{definition} The \textbf{reachable dependency poset} of a DSC is the result of an operation, \(rdp\), which sends DSCs to bounded posets (i.e. posets with top and bottom elements) by the following two-step procedure:  We take as elements all collections of events, i.e. \(\Pc(E)\), and impose the least order relation such that one collection of events, \(X\), is above another, \(Y\), if \(Y \subset X\) and for every element of \(X\), there is a possible dependency set contained in \(Y\).  The conditions of transitivity and cycle-freeness ensure that under such an ordering, every event will lie above the empty event set. We refer to event sets that exist in this poset as \textbf{reachable event sets}.
\end{definition}

Informally, a reachable dependency set is generated by asking "for each event, what are the basic (reachable) event sets which contain it," and then completing those by the empty set and all unions of this basis. This is sometimes known as "unwinding". Viewed as a graph, nodes of a reachable dependency poset correspond to complete event sets, and edges correspond to linear accretion of event sets over time by addition of subsequent events. Consequently, a reachable dependency poset may also be seen as generated by considering all possible dependency sets of all events, augmenting each with the event itself, and then, under the inclusion ordering, augmenting the result with the empty set and in addition all possible joins. We leave it to the reader to convince themselves that both procedures yield the same result.

It follows that for any DSC \((E,D)\), \(rdp((E,D))\)  is a subposet of \(\Pc(E)\), and which has all joins as unions. In fact, as a bounded poset with all joins, by the adjoint functor theorem for posets, it also has all  meets, and is hence a lattice.
However, importantly, meets do not correspond to intersections. Consider a DSC containing an event \(a\), which depends on either \(b\) or \(c\). \(\{a,b\}\) and  \(\{a,c\}\) are reachable event sets, but their intersection, \(\{a\}\), is not The meet is in fact \({}\). As a syntactic convenience, when we denote operations whose domain is a lattice on a DSC, we implicitly pass through the reachable dependency poset construction.

% TODO Show there is an inverse, also show this is a lattice

\section{Distributive Lattices and the Idempotent Distributive Lattice Completion}

We review here some basic facts and notation regarding order theory and lattices.

A \textbf{partially ordered set} or poset, \(P\) is a set equipped with a partial order relation \(\le\), which is transitive,  reflexive, and antisymmetric (i.e. for which \( a \le b \band b \le a \implies a = b\)). A (homo)morphism of posets is an monotone (order-preserving) function on their elements, and with such morphisms posets form the category \(Pos\) and finite posets form the subcategory \(FinPos\). Two posets are equivalent when there exist morphisms \(f, g\) between them such that \(f \odot g = id\) and \(g \odot f = id\), i.e. when they are equivalent as objects of \(Pos\). We note that all posets have a standard partial order on them such that \(P \le Q\) when there exists an order-preserving embedding \(P \rightarrow Q\).

A \textbf{lattice}, \(L\) is partially ordered set for which every two elements have a unique greatest lower bound, their \textbf{meet} (\(\wedge\)) and a unique least upper bound, their \textbf{join} (\(\vee\)). The join and meet operations of a lattice are necessarily commutative, associative, and idempotent. A (homo)morphism of distributive lattices is a morphism of posets which also preserves meets and joins. A \textbf{join-semilattice} and \textbf{meet-semilattice} are posets that respectively have all finite joins or all finite meets. A \textbf{complete lattice} is a lattice which has joins and meets of infinitary as well as finitary collections of elements. We write \(\bigwedge\) and \(\bigvee\) for the meet and join operations as applied to an entire set of elements. By abuse of notation, we also may write, e.g., \(x \vee S\) where \(x\) is an element of a poset and \(S\) is a set of elements, to indicate the lifting of application of the unary operation \(x \vee -\) to every element in the set.

A \textbf{distributive lattice}, is a lattice satisfying the additional property that for all \(x, y, z\) in \(L\), \(x \vee (y \wedge z) = (x \vee y) \wedge (x \vee z)\). It is easy to verify that if this condition (join distributing over meet) is satisfied, then the dual condition (meet distributing over join) is also satisfied.  Lattice homomorphisms between distributive lattices are necessarily distributive lattice homomorphisms, and with such morphisms distributive lattices form the category \(DLat\) and finite distributive lattices form the subcategory \(FinDLat\). In \(FinDLat\), all lattices necessarily have a unique top and bottom element (i.e. are bounded). As such, we require morphisms in \(FinDLat\) to also preserve top and bottom elements as the nullary join and meet (i.e. to be homomorphisms of bounded lattices).

A \textbf{join-irreducible} element of a poset is an element \(x\) such that no collection of elements not including \(x\) has \(x\) as its join. The operation \(\Jc(P)\) sends a poset (or a lattice viewed as a poset) to the sub-poset of its join-irreducible elements, sharing the same order relation. An intuition that this lends itself to is that join-irreducible elements are ideals. We refer to elements of a poset which are not join-irreducible as \textbf{composite} elements, and the set of join-irreducible elements which joins to them as their \textbf{basis}. It is important to note that if a poset has a globally least element (i.e. element which stands below all other elements in the order relation), that element is not join-irreducible, since it is the join of the empty set. However, if a poset has more than one locally least element (i.e. element with no element below it), then all such elements are join-irreducible. It is also important to note that even if an element is join-irreducible in \(P\), it still may nonetheless become a join in the restriction to \(\Jc(P)\).

A \textbf{downset} of a poset is a set of elements of the poset which is downwardly-closed -- i.e. for which \(x \in S \band y \le x \implies y \in S\). The operation \(\Oc(P)\) sends a poset to the poset of its downsets, ordered by inclusion. Such a poset has meets and joins as respectively intersection and union, and consequently is a distributive lattice. Further, \(\Oc(P)\) is a morphism (and in fact an embedding) of posets, which sends each \(x \in P\) to the set \(\{y \mathbin{|} y \le x\}\). The dual operation to taking downsets is taking \textbf{up-sets}  which are upwardly-closed. We denote this as \(\Ucc(P)\).

A \textbf{Heyting algebra} is a lattice with a unique top and bottom element, and a special "implication" operation called the \textbf{relative pseudo-complement} (\(a \rightarrow b\)) which yields the unique greatest element \(x\) such that \(a \wedge x \le b\). A \textbf{complete Heyting algebra} is a Heyting algebra such that it is also a complete lattice. The category of complete Heyting algebras takes as morphisms monotone functions which preserve finite meets, arbitrary joins, and implication.

A \textbf{frame} is a complete Heyting algebra. However, the category \(Frm\) of frames takes as morphisms monotone functions which preserve finite meets and arbitrary joins, but not necessarily implication. This is to say that the relative pseudo-complement operation derived from finite meets and arbitrary joins necessarily exists in frames, but may not commute with any given frame homomorphism. In the finitary case, distributive lattices and complete Heyting algebras coincide, and hence \(FinFrm = FinDLat\).

A \textbf{locale} is again the same thing as a frame. However, in the category \(Loc\) of locales, morphisms are viewed reversed, and hence \(Loc = Frm^{op}\) and \(FinLoc = FinFrm^{op} = FinDLat^{op}\)

In passing, we will make use of Birkhoff duality, which we recall here as well.

\textbf{Thm. (Birkhoff duality)}: When L is a finite distributive lattice, \(\Oc(\Jc(L)))\) is an equivalence, and for any finite poset P,  \(\Jc(\Oc(P)))\) is an equivalence. Further, this equivalence extends to a functorial equivalence between the categories \(FinPos\) and \(FinDLat\), with monotone functions on posets corresponding to homomorphisms of distributive lattices. As a consequence, all finite locales may be viewed in terms of their underlying posets.

\subsection{Distributive Lattices in Logic and Topology}

Distributive lattices play a very special role in both logic and topology. From a logical standpoint, Heyting algebras provide a complete semantics for intuitionistic logic, with true and false corresponding to top and bottom, and corresponding to meets, or corresponding to joins, and implication being given by the relative pseudo-complement. More precisely, a formula in intuitionistic propositional logic is provable (tautological) if and only if it is valid (yields true under every assignment of variables) in every Heyting algebra (and in fact this result holds even when one considers only finite Heyting algebras). Hence, a finite Heyting algebra (resp. distributive lattice) is a setting where we can directly interpret expressions in intuitionistic logic \cite{van1988troelstra}.

From a topological standpoint, frames are the object of study of locale theory, i.e. so-called "pointless topology." From any topological space -- given as a set of points, and a covering relation of open sets -- the open sets themselves form an order theoretic structure which is precisely a frame (which is known, when the homomorphisms between frames are viewed backwards, as a "locale"). If we then forget the points, and consider only the frame, we can still "do" topology -- and from any frame (resp. locale) we can recover a special type of space, known as a sober space. In fact, frames and sober spaces are in one-to-one correspondence \cite{johnstone1982stone, vickers1996topology}.

We will demonstrate that reachable dependency posets correspond one-to-one with lattices. This means that they clearly are not necessarily distributive. Thus they are not (yet) settings in which we can perform logical or topological analysis directly. However, intuitively, it feels like they \textit{should} be such a setting. In particular, events resemble points, and reachable event sets very much resemble open covers. This motivates the construction of the "best" way to derive an associated distributive lattice from a reachable dependency poset, and a further correspondence theorem that lets us establish an equivalence between DSCs and a special class of locales.

\subsection{Bruns-Lakser Completion}

In 1970, Bruns and Lakser introduced an indempotent distributive lattice completion for meet-semilattices \cite{bruns1970injective}. Only muchl later \cite{ball2016dedekind}, was it realized that this completion (though expressed in different terms) was actually introduced by  Holbrook MacNeille in the same 1937 work where he first introduced the famed Dedekind-MacNeille completion of partially ordered sets into complete lattices \cite{macneille1937partially}. 

First, we recall the original construction of Bruns-Lakser and MacNeille.

\begin{definition}
(Bruns-Lakser) An \textbf{admissible set} is a subset \(S\) of a meet-semilattice \(P\) in which for all \(x\), \(x \wedge \bigvee(S) = \bigvee(x \wedge S)\).
\end{definition}

\begin{theorem}
(Bruns-Lakser, MacNeille) The partially ordered set of all admissible sets of a meet-semilattice \(P\) is a distributive lattice, \(\BLc(P)\). There exists an injection \(bl : P \rightarrow \BLc(P)\), which preserves all meets and joins of admissible sets. Furthermore, any morphism to a distributive lattice that preserves all meets and joins of admissible sets, \(f : P \rightarrow D\), factors uniquely into the injection \(bl : P \rightarrow \BLc(P)\) followed by a distributive lattice homomorphism \(g : \BLc(P) \rightarrow D\).
\end{theorem}

The completion is idempotent. Further, it is functorial. \(\Mcc\) acts on morphisms that preserve meets and admissible joins (i.e. joins of admissible sets) by lifting their action on all elements in the original source and target, and extending their action on new elements in the source by sending them to the join in the target of the targets of their join-irreducible basis in the source. This functor in fact gives the category of distributive lattices as a reflective category of either the subcategory of meet-semilattices which shares all objects but only has morphisms that preserve meets and admissible joins \cite{gehrke2014distributive}.

% TODO give diagram.

The collection of admissible sets is rather large and unwieldy. But in the finite case, we have a much nicer characterization of the completion, which is computationally simple and also suggestive and familiar with regards to structures that occur elsewhere in computer science. The following is a simplification and extension of MacNeille's characterization of the finite elements of his completion, which holds even for posets which are not meet-semilattices:

\begin{lemma}
(Bazerman, Puzio [following MacNeille]) For a finite poset, \(\BLc(P)\) may be constructed as \(\Oc(\Jc(P)))\), with an injection that sends join-irreducible elements to their downsets, and composite elements to the union of their join-irreducible basis.
\end{lemma}

\subsection{Merkle Structures and the Completion of Reachable Dependency Posets}


\begin{figure}
\begin{minipage}[c]{0.3\textwidth}
\begin{equation*}
    \xymatrix{& abc \ar@{-}[dl] \ar@{-}[d] \ar@{-}[dr] & \\
      \underline{ab} \ar@{-}[d] \ar@{--}[dr] & bc \ar@{-}[dl] \ar@{-}[dr] &
        \underline{ac} \ar@{-}[d] \ar@{--}[dl] \\
        \underline{b} & \xcancel{a}  & \underline{c} }         
%      b \ar@{-}[dr] & \xcancel{a} \ar@{--}[d] & c \ar@{-}[dl] \\
%       & \emptyset &}
\end{equation*}
\end{minipage}
\begin{minipage}[c]{0.08\textwidth}
  \begin{equation*}
    \xymatrix{\ar@2{~>}[r]^{\Mcc} &}
  \end{equation*}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\begin{equation*}
    \xymatrix{& a_ba_cbc \ar@{-}[dl] \ar@{-}[dd] \ar@{-}[dr] & \\
      \mathbf{a_bbc} \ar@{-}[d] \ar@{-}[dr] & &
        \mathbf{a_cbc} \ar@{-}[d] \ar@{-}[dl]\\
        \underline{a_bb} \ar@{-}[d] & bc \ar@{-}[dl] \ar@{-}[dd] \ar@{-}[dr] &
          \underline{a_cc} \ar@{-}[d] \\
         \underline{b} \ar@{-}[dr] & & \underline{c} \ar@{-}[dl] \\
      & \mathbf{\emptyset} }
\end{equation*}
\end{minipage}
\caption{A simple example of the extended Bruns-Lakser completion, join irreducible elements (and their image) underlined, new elements in bold}
\label{Fig1}
\end{figure}

% TODO note that it extends to posets and this is made use of in the following section but not elsewhere.


To the vaguely formed question "how do we make reachable dependency posets topological" we can now propose a precise answer: application of the idempotent distributive lattice completion. This is to say, we take the composite \(\BLc(rdp(E,D))\) (which, by notational shorthand, may be written as \(\BLc(E,D)\)). The irreducible elements of a reachable dependency poset are those sets which are generated by the possible dependency sets of individual events -- i.e. that have an event which is shared by no complete dependency set below them. In the example where \(a\) depends on either \(b\) or \(c\), the irreducible elements, i.e. \(\Jc(rdp(E,D))\), are \(\{b\}\), \(\{c\}\), \(\{a,b\}\), and \(\{a,c\}\). Hence the application of \(\Oc\) yields the four sets \(\{\{b\}\}\), \(\{\{c\}\}\), \(\{\{b\},\{a,b\}\}\), and \(\{\{c\},\{a,c\}\}\), but also \(\{\}\), \(\{\{b\},\{c\},\{b,c\}\}\) and \(\{\{b\},\{c\},\{a,b\},\{a,c\}\}\) (see Figure \ref{Fig1}).

In the case of finite posets presented as a collection of sets, with ordering given by inclusion (as in the case of reachable dependency posets), we can give a simple algorithmic characterization of this construction. For every event which has multiple "paths" to enable it (i.e. multiple possible dependency sets), split the event into new events, each labeled by a different possible dependency set. And since branched events may depend on other branched events, do so recursively. In the resulting structure, rather than sets of events, there are sets of events each labeled by the "path" taken to get to them. 

From the standpoint of concurrent semantics, this amounts to replacing sets of events by sets of execution traces. From the standpoint of dependency specification, this amounts to augmenting packages by their "build plan". This operation makes intuitive sense in that it provides a more granular and correct specification of what a package "really" means. For example library \(a\) may link against library \(b\) or \(c\), each of which provide the same API-surface, but which have subtle differences in behavior. So while \(a\) depends "equally" on either \(b\) or \(c\), the resulting products, \(a_b\) and \(a_c\) are not guaranteed to be the same thing. It is precisely this distinction which is captured by taking the idempotent distributive lattice completion.

The subscript "shorthand" used above renders each event as subscripted by the path of dependencies to reach it. Since that path itself consists of events, in a nontrivial chain, then those events too are subscripted, and soforth. Representing this computationally seems a bit of a chore. If we took each label and turned it into a hash, and then when taking sets of labels instead took hashes of their hashes, etc, then (with high probability) we could represent the same information in constant space rather than space geometric in the height of our poset.

In fact, this operation -- augmenting a structure such that each element contains a hashed description of the path to reach it -- is relatively ubiquitous in modern software. It lies at the heart of the model of patches in distributed version control systems such as git, and is also used in blockchains. In those cases, while meets exist, joins do not. And further, in the blockchain case, it is even simpler because the goal is to restrict "truth" to a chosen linear path -- i.e. to avoid branching or "splits" in the chain. This construction is also the basic insight of the nix operating-system, as well as the "nix-like" store used by the Cabal build system for Haskell. In these cases, meets and joins both are used. The operation of generating the path information and taking a hash-chain of it is known as producing a \textbf{Merkle tree}\cite{merkle1987digital}. The idempotent distributive lattice completion, \(\BLc\) is then, in a sense, the \textit{non-probabilistic Merkle transformation} of a poset, and provides a formal description of what it means when we take an existing data structure and "turn it into a Merkle tree."

It would be remiss of us not to mention, as well, that the operation of sending elements of a poset to their downsets, a key step in this transformation, is a special case of the Yoneda embedding. In particular, it is the Yoneda embedding for (0,1)-presheaves -- i.e. when \(C^{op} \rightarrow Set\) is decategorified to \(P^{op} \rightarrow Bool\). Categorically-inclined readers may bear this in mind the next time somebody asks them what practical purpose such constructions serve.

%TODO get right functor arrow (notation)??

\subsection{The General Relationship between DSCs and Lattices}
Given that every DSC induces a finite lattice, it is reasonable to ask the general relation between finite lattices and DSCs. The following heuristic arguments provide motivation for our theorem. 

First, we consider the lattices induced by dependency structures with no choice -- i.e. where every event has a single possible dependency set. In such a case, the dependency structure presents a poset (and indeed, up to renaming, every poset induces a unique dependency structure). The reachable dependency poset of such a structure is generated by considering down-closed sets of events -- i.e. those where each event can occur only if all its dependencies occur. By Birkhoff duality, this is a distributive lattice, and thus choice-free dependency structures are equivalent to distributive lattices.

When choice appears, things become more complicated. Inspired by the Bruns-Lakser construction, we can consider a "splitting" of a DSC into a choice-free one which generates an equivalent distributive lattice under the Bruns-Lakser completion of the associated reachable dependency poset. The operation that performs this simply fragments each event with \(n\) possible dependency sets into \(n\) distinct events, each with a single dependency set. The original DSC is then recoverable, up to renaming, by a poset (of "split" events) along with a partition of the poset into elements that are the "same" event. Such a partition must have three conditions. First, the coherence condition -- all elements of a partition must share the same up-sets (i.e. anything which depends on a given fragment of an event must equivalently depend on any other fragment). Second, the antichain condition -- all elements of any given partition must be mutually unordered (because no event may depend on itself). Third, the distinction condition -- any two elements that share a partition must differ in their downset by at least one element besides themselves. This corresponds to the fact that we consider a \textit{set} of possible dependency sets rather than a \textit{multiset} of such. 

But suppose we were to lift these restrictions, and consider a "generalized DSC" as simply a poset coupled with an arbitrary partition. The reachable dependency poset of such a construction is then a quotient of the reachable dependency poset (i.e. downset dual) of the structure sans the partition relation. In particular, for any element (set of events), we quotient down all underlying elements that share a partition, except for the element (if any) that generated the given event set. This effectively runs the Bruns-Lakser completion in reverse. 

As an example, consider a single event which can depend on the empty event set three ways. The underlying poset is a three-branched tree. The Birkhoff dual of downsets yields a cube. And the partition quotient of this is the lattice M3 -- the "chinese lantern".

% TODO diagram

As another example, consider a single event which depends on the empty event set two ways, and one of those ways, can also depend on itself. I.e. \(a\) and \(a'\) depend on nothing, and \(a''\) depends on \(a'\). The underlying poset is a two-branched tree, one of whose branches itself has a branch. The Birkhoff dual of downsets yields two stacked diamonds. And the partition quotient of this, which identifies \(\{a,a'\}\) and  \(\{a,a',a''\}\) is the lattice N5 -- the pentagon.

% TODO diagram

M3 and N5 are the two "forbidden configurations" which may occur in general lattices, but not distributive lattices. This suggests that our "generalized DSCs" are closely related to general lattices. If we continue to drop the antichain condition, but retain the coherence and distinction conditions, we can still produce M3, but not N5. This suggests that "DSCs with multiplicity" (i.e. DSCs where we have a multiset of possible dependencies rather than a set of such) are closely related to modular lattices (which are characterized by having N5 as the single forbidden configuration). If we also retain the antichain condition, we cannot produce M3 directly, but we can produce larger constructions that contain it, as in Fig. \ref{Fig1}.

This motivation allows us to formulate a precise theorem. For lack of a better term, we say a lattice "has horizontal obstruction" if three elements share the same structure both below and above them (i.e. the precise same elements greater and less than them), as in M3. 

\begin{theorem}
\label{representation}
There exists a map \textbf{uds} (underlying dependency structure) from modular lattices with no horizontal obstruction to DSCs such that  \(rdp \circ uds\) is the identity, and \(uds \circ rdp\) is an equivalence up to renaming.
\end{theorem}

\begin{proof}
We construct \(uds(L)\) in two steps, first establishing the "free" DSC \(FD\) on the lattice \(L\), and then identifying events which are the "same". \(FD(L)\) has an event set consisting of the join-irreducible elements of \(L\), and as dependencies singleton sets freely generated by the ordering relation on the elements.  I.e. \(FD(L) = (\Jc(L), X \mapsto \{\{ y \in \Jc(L) \mid y<x\}\})\). To identify two events is to substitute all references for first to the second, and further, to define the new collection of possible dependency sets of the first to be the union of the existing collections given by both events. We denote a DSC \(D\) with two events, \(a\) and \(b\) identified as  \(D[a\sim b]\). The set \(Q(L)\) consists of pairs of join-irreducible elements \((a,b)\) such that  \(\exists\, x . \,  x < a \vee b \band x \vee a = x \vee b = a \vee b\).  Now, \(uds(L) = FD(L)[a \sim b,a' \sim b',...]\) for all \((a,b), (a',b'), ...\in Q(L)\).

Now we show that \(rdp \circ uds\) is the identity. When the source lattice is distributive, then the quotient operation trivializes. By Birkhoff duality, we indeed have the identity. When the source lattice fails to be distributive, we know that it is nonetheless modular, and so a failure will manifest as an instance of M3 as a sublattice, which will necessarily result in elements of \(Q(L)\). Given a pair in \(Q(L)\), considered as two event sets, the existence of a third "unifying" element (necessarily unordered with regards to them) could come about in two ways. Either, it could be a join irreducible element  as in M3, or it could be join reducible. However, if it were join irreducible, then either it would share the same structure below as well, or there would be another element below it with the same property that did share the same structure below. As such, the lattice would have horizontal obstruction. Therefore the element is necessarily join reducible, and since it is unordered with regards to the pair, it must be the join of two elements below the pair. Now, in the \(rdp \circ uds\) roundtrip, this "unifying" element will be discarded by \(uds\) because it is join reducible, and reconstructed by \(rdp\). Furthermore, the two "unified" elements will be quotiented into a single event with two possible dependency sets in one direction, and then "exploded" back into two distinct elements in the return trip. The rdp construction guarantees their structure below is preserved by such a roundtrip, and the fact that the two events were identified ensures that no additional structure above was created.

The above argument demonstrates that \(rdp\) generates all horizontal-obstruction-free modular lattices. To complete the proof we must now show that \(rdp\) generates only such lattices. To fail to be modular, there would need to exist three generated elements, \(a, b, x\) such that \(a < b\) and \(a \vee (x \wedge b) \neq (a \vee x) \wedge b\). In the \(rdp\) construction such elements correspond to event sets, and join is set union (while meet may have more structure). So we would need an event set \(b\) with a subset \(a\) such that if we take  \(x \wedge b\) and then union with \(a\) the result is different than if we first union with \(a\) and then meet \(b\). If \(a\) is lower than \(x \wedge b\), then the equality holds trivially. If it is unordered with regard to \(x \wedge b\) , then the meet of the two must be \(b\), and again the equality holds. Hence, we are concerned with the case when it is higher than the join. But in such a case, \(a \vee x\)  must meet \(b\) precisely at \(a\), since the meet is the first "valid" subset of the intersection, and we already know \(a\) is valid. And since \(a\) lies above \(x \wedge b\) , we know that too is \(a\), and hence we know the lattice must be modular.

Finally, we demonstrate that \(rdp\) must generate lattices that are free of horizontal-obstruction.Two events can share the same structure above if everything which depends on one must also depend on the other. However, since these events must be relatively unordered, a third element sharing the same structure above would invalidate that condition on one of the first two. Hence, for three elements to share the same structure above, one must be join-reducible. In such a case, these elements would then not share the same structure below.

% TODO check if this suffices as a proof, improve.
\end{proof}


\subsection{DSCs and Finite Locales}

We have seen how a DSC may be transformed into a finite locale via the extended Bruns-Lakser completion of its reachable dependency poset. It is natural to ask how far this relationship extends. It is almost immediate that in fact any finite locale may be generated by a DSC. All that is necessary is to consider the join-irreducible elements of the frame as events, each of whose single possible dependency set is all other events below it in the frame ordering. However, even up to renaming of events, distinct DSCs can nonetheless present order-equivalent frames. For example, the DSC with \(a\) depending on \(b\) or \(c\) gives the same frame as the DSC with \(a\) depending on \(b\) and \(d\) depending on \(c\) (since the Bruns-Lakser completion "splits" the former \(a\) into two copies to begin with). 

This tension (multiple presentations of the same structure) occurs very frequently in topos theory (and its decategorification in locale theory) when multiple sites (categories with a topology, resp. posets with a coverage) can present the same topos (resp. locale). This suggests that a tighter correspondence should not be to locales directly, but rather to their presentations as sites (known, for locales, as "posites"). A good overview of posites is provided by \cite{schultz2017temporal}.

The central tool in this section is the nucleus, which is the localic analogue of a topology. We recall its definition and some basic facts regarding it. Readers are referred to \cite{johnstone1982stone, vickers1996topology} for further discussion.

A \textbf{nucleus} is the algebraic structure on a frame that gives a sublocale. It given as a monotone function on a frame \(j : L \rightarrow L\), satisfying three properties. First: \(j(a \wedge b) = j(a) \wedge j(b)\). Second, \(a \le j(a)\). Finally, \(j(ja) \le j(a)\). These may be summarized as saying that it is (finite) meet-preserving, contractive (in this case, inflationary) and idempotent. An element of \(L\) is said to be \(j\)-closed if \(j(a)=a\). Further, if \(L\) is a frame, then \(L/j\), which consists only of \(j\)-closed elements, is also a frame, and there exists a surjective frame homomorphism \(j^* : L \rightarrow L/j\). If we view a frame as a category, a nucleus is just a left-exact idempotent monad; if we view a frame as a decategorified topos, a nucleus is a topology; and if we view a frame as generated by its internal logic, a nucleus gives a "possibility" or "locally true" modal operator on that logic, analogous to the S4 diamond.

A site of a topos is typically given as a category and an associated Grothendieck topology. But such topologies are somewhat painful to manipulate and reason about. In the special case where we are concerned only with the "(0,1)-sheaves" (i.e. truth-valued sheaves, i.e. downsets) of a poset, we can equally well just work with a site as a poset and a nucleus on its downsets.

\begin{definition}
The \textbf{Bruns-Lakser topology} on a finite locale is a function generated by the following stepwise procedure. First we consider the join-irreducible elements of the underlying poset of the locale (i.e. the poset of its join-irreducible elements). We term such elements the "double basis" of the locale. For double-basis elements, we define \(bl\) as the identity. For all joins of such elements, \(bl\) is again the identity. And for all meets of all elements thus far enumerated, \(bl\) is again the identity. For all other elements, \(bl\) is the meet of all idempotent elements greater than or equal to it. Since we have explicitly added all necessary meets, the result is necessarily an idempotent.
\end{definition}

\begin{lemma}
The poset endomorphism, \(bl\), generated by the join-meet completion of the join-irreducible elements of the basis of a distributive lattice is a nucleus.
\end{lemma}
\begin{proof}
Since \(bl\) is idempotent by construction and obviously contractive, we only need show it preserves meets; i.e. that that \(bl(x) \wedge bl(y) = bl(x  \wedge  y)\).  If the meet is a meet of idempotents, then it is preserved by construction. If it is not, then it is the meet of elements which themselves are the meets of idempotents. As such, it may be rewritten to be a meet of idempotents, and thus is also preserved.
\end{proof}

%TODO not so obvious it is idempotent and contractive! Idempotent -- everything gets sent to one. Contractive -- show that for nonidmepotents, they go up!

\begin{lemma}
The Bruns-Lakser topology is the least topology which preserves the join-irreducible elements of the underlying poset of a finite frame.
\end{lemma}
\begin{proof}
We consider a finite frame as given by its underlying poset, i.e. as \(\Oc(P)\). Clearly \(\Oc(P)/bl\) contains elements corresponding to all join-irreducible elements in \(P\), and they remain join-irreducible as no elements are added below them. Further, as it is a frame, it is a distributive lattice. It remains to show that it is the least such lattice. The only idempotents which could introduce new join-irreducible elements in the induced basis are the meets of existing elements, of the form  \(z = x \wedge y\). Then \(z = (a \vee b) \wedge (c \vee d)\) for some four elements (possibly overlapping) which are themselves join-irreducible or joins of join-irreducible elements.  By the laws of distributive lattices we may rewrite between disjunctive and conjunctive normal forms and so \(z = \bigvee \{a \wedge c, a \wedge d, b \wedge c, b \wedge d\}\).  But now it is a join of smaller meets. Inductively, either these meets are join-irreducible themselves, or they may be further iteratively decomposed until they are, and therefore the original element is a join of irreducible elements.
\end{proof}

We can now state the central theorem of this section:

\begin{theorem}
The set of DSCs is one-to-one with the set finite posites generated by downsets of horizontal-obstruction-free modular lattices, and equipped with the Bruns-Lakser topology.
\end{theorem}
\begin{proof}
The bulk of the work is handled by \ref{representation}, which establishes the connection between DSCs and horizontal-obstruction-free modular lattices. The correspondence between these and their downsets is established by Birkhoff duality, and the construction of a locale by Bruns-Lakser by the above lemma.
\end{proof}

\section{The Modal Logics of Dependency Structures}
\subsection{Version Parameterizations of Dependency Structures and their Posets}

The mapping from an actual repository to a DCS is incomplete in that it throws away information about two packages being different "versions" of the same thing. Here we introduce a data structure that captures that additional information. A higher version of a package is, intuitively, something that is "almost the same, but better." Similarly, in a DSC, some events are in a sense "universally more powerful" than other events, in that they enable at least all the things enabled by the events they cover. We say an event is a "higher version" of another event if for possible dependency set of every event containing the lower event, there is also a possible dependency set for that event which differs only in that it contains the higher event instead.

\begin{definition}
A \textbf{version parameterization} of a DSC is an idempotent endofunction on events, from lower to higher, satisfying the above criteria; i.e. where for every possible dependency set of every event, there is another possible dependency set of that event where the lower versions have been substituted for higher versions. Idempotency here translates into the condition that no higher event is itself a lower event of something else.
\end{definition}

%TODO use symbols

Versioning parameterizations on DSCs in turn give rise to related structures on their reachable dependency posets. Since a reachable dependency poset is complete under joins, for every two event sets differing only in versions of some events, we can take the element corresponding to the union of their event sets. This yields a  endofunction on elements of the reachable dependency poset, sending ("contracting") the lower and higher event sets both to their corresponding union. We term such an endofunction a poset version parameterization.

\begin{definition}
A \textbf{ponucleus} is an idempotent monotone endofunction on a poset which preserves existing finite meets.
\end{definition}

\begin{lemma}
Every poset version parameterization is a ponucleus that in addition preserves existing joins.
\end{lemma}

\begin{proof}
Clearly this function is idempotent and monotone by construction. It remains to show it preserves meets and joins.

First we consider meets. If two sets had a meet before, and one is now "contracted" to a higher version, this may introduce new elements in the intersection only on the condition that these elements were also in the second set. But that would mean that the elements of the second set would also be contracted in the same way, as would the elements in the intersection itself.

Next, we consider joins. If two sets had a join before, and one was contracted to a higher version, then this would introduce new elements in the union. But those elements are the same element which are introduced by contracting the union directly.
\end{proof}

Since poset version parameterizations preserve existing meets and joins, they certainly preserve meets and maximal joins, and thus are acted on by \(\BLc\).

\begin{theorem}
Given a join-preserving ponucleus \(j\), on a poset \(P\),  \(\BLc(j)\) is a nucleus on \(\BLc(P)\)
\end{theorem}
\begin{proof}
First we consider meet preservation. Since we know that \(j\) preserves meets, we only need observe that the action of \(\BLc\) on morphisms preserves meet-preservation. This follows from the fact that \(\BLc\) itself preserves meets.

%TODO check, spell out more.

Next, we consider contractivity. We already defined \(j\) to be contractive. So now we only need observe that \(\BLc\) preserves contractivity. For this to fail, it would need to extend a morphism so that some element was mapped to something below itself. But since contractivity is preserved for the basis join-irreducible elements, it must be preserved for all elements.

Finally, we consider idempotence. Again, this is given by idempotence of \(j\) combined with \(\BLc\) preserving idempotence of join-irreducible elements.
\end{proof}

As a corollary of the above, given a DSC \((E,D)\) and a version parameterization with an induced poset version parameterization \(p\), then \(\BLc(p)\) is a nucleus on \(\BLc(E,D)\).


\section{Free Distributive Lattices and Logic}
As discussed above, is part of the basic theory of Heyting algebras that they possess an internal intuitionistic logic. Here we sketch how it works in our particular case.

Given a DSC \((E,D)\), we construct a language consisting of atoms given by join-irreducible elements of the reachable dependency poset (which may be thought of, as above, as events subscripted with their dependency trace), completed under the standard logical connectives. Every formula in this language corresponds to a particular element in the Merkle-lattice of our DSC, \(\BLc(E,D)\).  Conjunction corresponds to meet, disjunction to join, and implication to the relative pseudo-complement. This is a logic of reachable states of our system, and their traces, which describes all possible states of the system as disjunctions of join-irreducible states. Given two event sets, considered as reachable states, disjunction gives the set of events that have occurred in either state. Conjunction gives the set of events which have occurred in both states.

%TODO explain what relative pseudo-complement interprets as in this setting 

The pvp-induced nucleus discussed above equips the internal logic of \(\BLc(E,D)\) with a modal operator. We can interpret this operator as "round (or upgrade) to the highest version," and give corresponding interpretations of the meaning of its basic laws. \(x \rightarrow \Dia{x}\) tells us that everywhere \(x\) is valid, so too is its highest version (which is precisely how we constructed our modality to begin with). \(\Dia\Dia{x} \rightarrow \Dia{x}\) tells us that the highest version of the highest version is just the highest version. \((x \rightarrow y) \rightarrow (\Dia{x} \rightarrow \Dia{y})\)  tells us that if an implication holds for an event set, then the highest version of that set implies the highest version of the consequent. Finally, \(\Dia(x \band y) = \Dia{x} \band \Dia{y}\) tells us that the highest version of a conjunction may be computed as the conjunction of the highest versions of its constituents.

In fact, the this modality is powerful enough to yield an internal "bind" operator. Since we have a strength that gives \(x \band \Dia{y} \rightarrow \Dia(x \band y)\), and since we have an internal evaluation \(x \band (x \rightarrow y) \rightarrow y\), then we have \(\Dia{x} \band (x \rightarrow \Dia{y}) \rightarrow \Dia{y}\), read as "if we know that some version of x implies the highest version of y, then to imply y it suffices to consider the highest version of x." Hence, we have a computational monad in the sense of Moggi \cite{moggi1991notions}, and rounding (or "upgrading") is an "effect".

% Show that this is not true for disjunction -- counterexample.

From the standpoint of nondeterministic concurrent semantics, all of this is perfectly reasonable. From two configurations (considered as nondeterministic traces), "or" gives us their combination, and "and" gives us the least trace which they share between them. However, from the standpoint of dependencies, this is insufficient. It is a logic of states in the system, but it is not a logic \textit{about} states in the system -- i.e., requirements. A a logical "or"  would express that we're asking for one or the other event set as such, unlike the disjunction given here. Similarly, a logical "and" would express that we are requiring all events in both sets (i.e. what disjunction actually provides). Without this missing logical "or", for example, we cannot give a formula that specifies "any way to reach an event, regardless of the dependencies," as in the logic thus far, \(a_b \bor a_c\) specifies \textit{both} ways, as opposed to \textit{either} way.

Some further mathematical constructions are required to build back up to the logic we'd really like. Returning to the finite characterization of the extended Bruns-Lakser completion, we note that taking the downsets of the join-irreducible elements is effectively taking their free join-completion. Dually, taking up-sets is a free meet-completion. In fact, for a discrete set \(S\), \(\Ucc(\Oc(S))\) is the \textbf{free distributive lattice} over S, which is a well studied mathematical object \cite{gratzer2009lattice}. The elements of this belong to \(\Pc(\Pc(S)\) and consist of its \textit{irredundant subsets}. This is to say that we can read these elements as logical expressions in disjunctive normal form, for example as \(ab \bor c\). An irredundant set is one in which the clauses have been simplified -- i.e. in which \(a \bor ab\) has been reduced to simply \(a\).  In this construction, join remains union -- but now it is not a union of sets of atoms, but a union of \textbf{sets of sets} of atoms. Meet however, is no longer intersection. Rather, it becomes convolution, just as in standard logic! This is to say that the meet (and) of \(a \bor b\) and \(c \bor d\) becomes the irredundant (simplified) core of \(a \band c \bor a \band d \bor b \band c \bor b \band d\).

There are some interesting open problems regarding free distributive lattices, in particular the search for a closed form expression that counts the elements of such a construction over a set of a given size (on which more later).

The free distributive lattice construction extends to any poset \(P\), where the up-sets of the downsets are the free distributive lattice of a poset. This is a less studied, but still known construction \cite{johnstone1982stone}. As above, join is union and meet is convolution. But when we "multiply" two atoms, we don't simply conjoin them. Rather, we take their join in the underlying poset. Hence we arrive at a quotient of the free distributive lattice generated when the elements of \(P\) are considered simply as a set, and potentially a much smaller one. For example, if the underlying poset is a linear order, then the free distributive lattice over it is equivalent to the original poset. In general, the more ordering in the original poset, the smaller the resultant free distributive lattice over it.

 In line with this, we can build the free meet-completion of the idempotent distributive lattice completion of a dependency poset by the compound construction \(\Ucc(\Oc(\Jc(P)))\). For a DSC, the internal logic of this construction, which we call \(\UBLc(E,D)\) has precisely the same syntax as that of the internal logic \(\BLc(E,D)\). However, while the normal form of the latter consisted of a set of irreducible elements, the normal form for our new construction consists of a set of sets -- i.e. a boolean expression of these elements given in disjunctive normal form, and simplified by reduction with regards to redundancy as well as the ordering relationship given on the join-irreducibles. We refer to the elements of \(\UBLc(E,D)\) not as event sets, but as event equations, since they consist of events related by conjunction and disjunction. Further, it should be noted that since \(\BLc\) is functorial, and taking up-sets is functorial, then the compound \(\UBLc\) is also functorial, and has an action on morphisms of posets.
 
%TODO illustrations

%TODO explain what relative pseudo-complement interprets as in this setting  ?

To fully explore the dependency standpoint. we would also like to interpret the modal operator associated with a versioning parameterization in \(\UBLc(E,D)\) as well. This brings us to the following:

\begin{theorem}
Given a DSC \((E,D)\) and a version parameterization with an induced poset version parameterization \(p\), then \(\UBLc(p)\) is a nucleus on \(\UBLc(E,D)\). Furthermore, it preserves not only meets, but also joins.
\end{theorem}
\begin{proof}
We have already established that \(\BLc\) yields a contractive endofunction on a distributive lattice (and more). Since taking up-sets is functorial, we know that this in turn yields a contractive endo-function on the compound. Furthermore, since the up-set functor runs from all posets to distributive lattices, it lifts order-preserving morphisms of posets into meet and join preserving morphisms of distributive lattices. Hence, the induced contractive endofunction preserves both meets and joins, and is a nucleus.
\end{proof}

This in turn induces a modality, as above, but now on the logic not only of event sets, but of event equations. If we view reachable configurations as nondeterministic traces, then the "highest version" is most general state with regards to any given configuration -- i.e. associates to any trace another one which provides at least as many options for further execution as before, and possibly more. The various laws can be read similarly. %TODO check intuition, give example?

Thus we get a simple type theory of package dependencies with an internal "version policy" modality that obeys the usual algebraic identities, and lets us treat contraction along the version policy as a monadic effect. Package version policies have been a subject of some debate in the applied world. But as we now see, all told, a package version policy in a dependency structure is just a certain type of monad in the free distributive lattice over the join-irreducibles of the associated poset.

% Free Distributive Lattices and Nuclei

\section{Solutions to Dependency Problems, and their Combinatorial Properties}
In future work, we will discuss a general theory of incompatibilities. For now, we define a general notion of a dependency problem.

\begin{definition}
A dependency problem in a DSC \((E,D)\) is the pair of a formula \(\phi\) in \(\UBLc(E,D)\) and a monotone increasing (i.e. growing as further elements are added to the source set) objective function of type \(\Pc(E) \rightarrow \mathbb{R}\). A solution to such a problem is an event set which satisfies the formula and minimizes the objective function.
\end{definition}

This naturally encodes many problems. For example, it allows us to calculate the minimal dependencies needed to reach a certain state. Further, if we associate a cost function which counts the number of incompatibilities or conflicts in a given event set, then a conflict-free solution is possible when a minimal solution has a cost of zero.

Even without further constraints on the objective function, solving a dependency problem does not mean undertaking a brute force search over all event sets which satisfy the formula. In particular, since the objective function is monotone, we need only examine the minimal event sets which satisfy it. These form a maximal  antichain -- i.e. they are unordered with relation to one another, but every other point on the lattice is ordered with regards to at least one such set. The former property follows immediately from monotonicity. The latter comes from the fact that the top element in the lattice satisfies all formulae in our language (since it has no notion of negation), and hence every set which does not satisfy \(\phi\) is ordered in relation to at least one set which does.

This encourages us to focus on the efficiency of enumerating maximal antichains, and in particular the maximum size of an antichain within a poset -- known as the width of the poset, which we denote as \(\Wf(P)\). The downsets over a discrete set have the special property that maximal antichains are one-to-one with formulae in that set's elements, modulo the equational rules of first order logic. This follows from the fact that if a formula specified two ordered elements, one would be redundant with regards to the other and further, every element either does or does not satisfy a given formula. In the downsets over a more structured poset, cuts remain one-to-one with formulae -- but modulo not only the standard rules of logic, but also those relations induced by the underlying poset. Hence, in general, the maximum width of the collection of downsets over a poset is the same as the maximum number of disjuncts in a formula over the elements of the poset presented in disjunctive normal form, and modulo appropriate relations. In fact, we have the following lemma, which is straightforward, but we have not seen recorded in existing literature:

\begin{lemma}
The collection of all antichains in the downsets of a finite poset \(P\), under the inclusion ordering, corresponds to the free distributive lattice over \(P\).
\end{lemma}
\begin{proof}
We have already defined the free distributive lattice as the up-sets of the downsets. Hence we need only show that up-sets are one-to-one with antichains. In a finite lattice, every up-set corresponds to a unique set of basis elements, which are unordered with respect to one another, and hence form an antichain. Conversely, every antichain thus generates a distinct up-set.
\end{proof}

By Sperner's Theorem, first published in 1928, the powerset of a set with \(n\) elements, under inclusion ordering, has a width of \(\binom{n}{\ceil{n/2}}\). (Because this picks out the central elements of Pascal's triangle, using ceiling or floor here yields the same sequence. For reasons that will become clear, contrary to standard convention, we prefer to use ceiling). In the other extreme, when all elements of the underlying poset are strictly ordered, then all elements of the resultant downset are also strictly ordered, and hence the width is 1. This yields a key insight -- the complexity of solving a dependency problem is not strictly a function of the quantity of events -- rather, it is jointly determined by the quantity of events and the degree of dependency induced by their underlying topological structure. Sperner's Theorem lies at the foundation of a field of combinatorics known as extremal set theory and is closely related to the methods and tools of algebraic combinatorics. Using techniques from these fields, we can give bounds that capture this general relationship between width, size, and dependency degree, such as the following result (proof in Appendix B).

\begin{theorem}
Define \(\Mf(a,b)\) as the central (maximal) coefficient of the formal polynomial expansion of \((1 + x + x^2 ... + x^a)^b\). (When \(a\) is 2, this is the central binomial coefficient, as appearing in Sperner's Theorem). Define \(\Hf(P)\) as the height of a poset, i.e. the length of its longest chain. Given any poset \(P\), then \(\Wf(\Oc(P)) \le \Mf(2,\Wf(P)) * \Mf(\Hf(P),\ceil{\Wf(P)/2})\).
\end{theorem}

When a poset is discrete (i.e., a set) then this collapses to a statement of the central inequality of Sperner's Theorem.

% Question: are all poset version parameterizations / exact coverages generated by covering relations in an underlying dsc? Which ones are?

% \begin{proof}
% Dilworth shows w(p) is the same as maximal chains in p. Downsets of p correspond to at most one element of each chain. The antichain consists of downsets generated by n/2 chains. Given a choice of n/2-many specific chains, one can produce sperner-many order-unrelated downsets.
% \end{proof}

% Relationship of this inequality to pvps? Show that width can be calculated as the maximal nonidempotent elements across any poset version parameterization over the free semilattice completion of a poset.


% \begin{lemma}
% Every poset has a unique maximal poset version parameterization, which acts as the identity on elements iff they are nondegenerate.
% \end{lemma}

% Conj, width is homotopyical invariant at specific number of zero cells -- Nope

% show that pvps have a maximal one. further, show that it consists of nondegenerate join irreducible elements.

% Nondegenerate join irreducible elements. i.e. ones which do not factor through something they are greater than.


% https://en.wikipedia.org/wiki/Sperner%27s_theorem
% http://oeis.org/A001405
% https://www.ams.org/journals/bull/1988-19-02/S0273-0979-1988-15725-4/S0273-0979-1988-15725-4.pdf
% Alexander -- combinatorics of finite sets
% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.655.7131&rep=rep1&type=pdf


% Prove that for a modality, the difficulty of solving <>(phi) <= difficulty of solving phi.
% for a discrete set, difficulty of solving is bounded by double exponential
% equipping set with structure -- degenerate case is linear. Question: what formula if any relates structure to cost
% Question: what if we stratify by tree depth instead of node quantity (or all at once)? i.e. what is general formula for efficiency?
% Relate to np completeness

% Compare structure as one-cells vs structure as the poset of a globular complex.


\section{Related and Future Work}
% In fact, since \(\Dia\) commutes with meets (i.e. \(\Dia(x \band y) = \Dia{x} \band \Dia{y}\) and  \(\Dia(x \bor y) = \Dia{x} \bor \Dia{y}\))
% Look at paper ray sent on physics -- sorkin, penrose.
% Structure of mathematical proofs, knowledge

% Directed topology

%
% Temporal logic modalities for internal logic? "eventually, always, soon" -- i.e., below, above, close
% translate a theory (a  la smt) into topological knowledge for the formula itself, and hence get a parameterizable smt solver
% a, b elem G. a * a = a, b * b = b -- 
% http://drops.dagstuhl.de/opus/volltexte/2019/10553/pdf/LIPIcs-SNAPL-2019-10.pdf
% https://ths.rwth-aachen.de/wp-content/uploads/sites/4/teaching/vorlesung_satchecking/ws14_15/04a_equality_handout.pdf

%Focus on compositional semantics of concurrent processes has _necessarily_ led to simplifying assumptions / enforced behaviors in process calculi, because trace semantics of concurrent programs do not compose. This is because trace semantics are only the 1-cells of a more complicated topological object. What composes is the full cellular structure, including the higher structure of incompatibilities, 2-incompatibilities, etc.

% https://www.kosmikus.org/SemanticsOfVersionControl/
% Other VCS stuff -- malbos, mimram, hott
% concurrency stuff
% solve conflict stuff

% Relatiionship to SAT solving. phase transition = width. dnf vs cnf.

% RELATED WORK
% Subsumption lattice


 \nocite{*}
 \bibliographystyle{amsalpha}
 \bibliography{topo-logic-draft}

\end{document}


% Nielsen, M., Plotkin, G., Winskel, G., Petri nets, Event structures and Domains, part 1. Theoretical Computer Science, vol. 13 (1981).
% heyting algebras: http://www2.math.uu.se/~palmgren/tillog/heyting3.pdf
% also A.S. Troelstra and D. van Dalen (1988).Constructivism in Mathematics, Vol. I &II.North-Holland.

% BL
% The Dedekind MacNeille site completion of a meet semilattice
% Richard N. Ball, AleÅ¡ Pultr, Joanne Walters Wayland

% https://arxiv.org/abs/1309.3113 "Distributive envelopes and topological duality for lattices via canonical extensions" -- gherke, van gool
% Grothendieck topologies on posetsâ A.J. Lindenhovius


% Incompatibilities and knowledge representation

% Program to calculate dependency coverings of hackage.

% Logi-computational aspects -- Single-Implicative Normal Form of equations. From cnf, each atom -> all clauses not containing it, anded. From DNF -> each atom to all clauses containing it, ored. SAT is send a formula to SINF (forgetting polarity relations) then under polarity relations, trying to discover if there is any assignment of truth values such that the incompatibility function does not proc.



% Thank. James, Jay, Tom. SPJ, HVR -- early inspiration. MIT categories seminar, spivak. Richard Stanley.

% Cite SHAKE PAPER